{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T21:33:11.616991Z",
     "start_time": "2024-10-22T21:33:09.923316Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from itertools import product\n",
    "import scikit_posthocs as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def read_csv(folder_path, dataset_name, reduction_mode):\n",
    "    if reduction_mode:\n",
    "        csv_files = [f for f in os.listdir(folder_path) if (dataset_name in f and reduction_mode in f and 'all' not in f)]\n",
    "    else:\n",
    "        csv_files = [f for f in os.listdir(folder_path) if (dataset_name in f and 'all' not in f)]\n",
    "    df_list = []\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_list.append(df)\n",
    "\n",
    "    df_combined = pd.concat(df_list, ignore_index=True)\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e55c6a085ffce4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T21:58:51.939428Z",
     "start_time": "2024-10-22T21:58:51.892540Z"
    }
   },
   "outputs": [],
   "source": [
    "def aggregate_metrics(df_combined, method):\n",
    "    if method == 'knn':\n",
    "        columns = ['K', 'Distance', 'Voting scheme', 'Weight scheme']\n",
    "    elif method == 'svm':\n",
    "        columns = ['Kernel']\n",
    "    \n",
    "    grouped_df = df_combined.groupby(columns)\n",
    "\n",
    "    # Compute mean and standard deviation of the relevant metrics\n",
    "    metrics_summary = grouped_df.agg({\n",
    "        'Accuracy': ['mean', 'std'],\n",
    "        'Precision_Class_0': ['mean', 'std'],\n",
    "        'Recall_Class_0': ['mean', 'std'],\n",
    "        'F1_Class_0': ['mean', 'std'],\n",
    "        'Precision_Class_1': ['mean', 'std'],\n",
    "        'Recall_Class_1': ['mean', 'std'],\n",
    "        'F1_Class_1': ['mean', 'std'],\n",
    "        'Solving Time': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "\n",
    "    # Rename the columns for clarity\n",
    "    metrics_summary.columns = columns + [\n",
    "                            'Accuracy_mean', 'Accuracy_std',\n",
    "                            'Precision_Class_0_mean', 'Precision_Class_0_std',\n",
    "                            'Recall_Class_0_mean', 'Recall_Class_0_std',\n",
    "                            'F1_Class_0_mean', 'F1_Class_0_std',\n",
    "                            'Precision_Class_1_mean', 'Precision_Class_1_std',\n",
    "                            'Recall_Class_1_mean', 'Recall_Class_1_std',\n",
    "                            'F1_Class_1_mean', 'F1_Class_1_std',\n",
    "                            'Solving Time_mean', 'Solving Time_std']\n",
    "\n",
    "    metrics_summary = metrics_summary.sort_values(by='Accuracy_mean', ascending = False)\n",
    "\n",
    "    # Get the best hyperparameters (the first row after sorting)\n",
    "    best_hyperparams = metrics_summary.iloc[0][columns].to_dict()\n",
    "    best_accuracy = metrics_summary.iloc[0]['Accuracy_mean']\n",
    "\n",
    "    # Print the best hyperparameters and their accuracy\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    print(best_hyperparams)\n",
    "    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
    "    \n",
    "    return metrics_summary, best_hyperparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b4f4a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_knn(best_models, df_combined):\n",
    "    metric = 'Accuracy'\n",
    "    metric_values = []\n",
    "    for _, row in best_models.iterrows():\n",
    "        # Store model identification (e.g., hyperparameters)\n",
    "        model_id = (row['Voting scheme'], row['Weight scheme'], row['Distance'], row['K'])\n",
    "\n",
    "        # Filter for the current hyperparameter combination\n",
    "        filtered_df = df_combined[\n",
    "            (df_combined['Voting scheme'] == row['Voting scheme']) & \n",
    "            (df_combined['Weight scheme'] == row['Weight scheme']) &\n",
    "            (df_combined['Distance'] == row['Distance']) &\n",
    "            (df_combined['K'] == row['K'])\n",
    "        ]\n",
    "        # Collect accuracy values for this combination\n",
    "        metric_values.append(list(filtered_df[metric].values))\n",
    "    \n",
    "    return metric_values\n",
    "\n",
    "def get_metrics_svm(best_models, df_combined):\n",
    "    metric = 'Accuracy'\n",
    "    metric_values = []\n",
    "    for _, row in best_models.iterrows():\n",
    "        # Store model identification (e.g., hyperparameters)\n",
    "        model_id = (row['Kernel'])\n",
    "\n",
    "        # Filter for the current hyperparameter combination\n",
    "        filtered_df = df_combined[\n",
    "            (df_combined['Kernel'] == row['Kernel'])\n",
    "        ]\n",
    "    # Collect accuracy values for this combination\n",
    "        metric_values.append(list(filtered_df[metric].values))\n",
    "    \n",
    "    return metric_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "800d9f11d23f13ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T22:01:24.275420Z",
     "start_time": "2024-10-22T22:01:22.361594Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluation_test(metric_values):\n",
    "\n",
    "    # Perform the Friedman test if there are three or more models\n",
    "    if len(metric_values) >= 3:\n",
    "\n",
    "        stat, p_value = stats.friedmanchisquare(*metric_values)\n",
    "        print(f\"Friedman test statistic: {stat}, p-value: {p_value}\")\n",
    "\n",
    "        if p_value < 0.05:\n",
    "            print(\"Significant differences found, conducting Nemenyi post-hoc test\")\n",
    "            data = pd.DataFrame(metric_values).T  # Transpose so each column is a model\n",
    "            nemenyi_result = sp.posthoc_nemenyi_friedman(data)\n",
    "            print(nemenyi_result)\n",
    "        else:\n",
    "            print(\"No significant differences found between the models.\")\n",
    "    \n",
    "    # Perform the Friedman test if there are two models\n",
    "    elif len(metric_values) == 2:\n",
    "\n",
    "        stat, p_value = stats.wilcoxon(metric_values[0], metric_values[1])\n",
    "        print(f\"Wilcoxon signed-rank test statistic: {stat}, p-value: {p_value}\")\n",
    "\n",
    "        if p_value < 0.05:\n",
    "            print(\"Significant difference between the two models.\")\n",
    "        else:\n",
    "            print(\"No significant differences found between the two models.\")\n",
    "    else:\n",
    "        print(\"Not enough data to perform the test.\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b5b0dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_t_test(metric_values1, metric_values2):\n",
    "\n",
    "    stat, p_value = stats.ttest_rel(metric_values1, metric_values2)\n",
    "    print(f\"Paired t-test statistic: {stat}, p-value: {p_value}\")\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        print(\"Significant difference found between the two models.\")\n",
    "    else:\n",
    "        print(\"No significant differences found between the two models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5751feab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating method knn on dataset: grid\n",
      "Best Hyperparameters:\n",
      "{'K': 7, 'Distance': 'minkowski2', 'Voting scheme': 'Majority_class', 'Weight scheme': 'Mutual_classifier'}\n",
      "Best Accuracy: 0.9672\n",
      "Friedman test statistic: 3.857988165680463, p-value: 0.4255660834829863\n",
      "No significant differences found between the models.\n",
      "\n",
      "\n",
      "Evaluating method svm on dataset: grid\n",
      "Best Hyperparameters:\n",
      "{'Kernel': 'rbf'}\n",
      "Best Accuracy: 0.9142\n",
      "Wilcoxon signed-rank test statistic: 0.0, p-value: 0.001953125\n",
      "Significant difference between the two models.\n",
      "\n",
      "\n",
      "Evaluating method knn on dataset: sick\n",
      "Best Hyperparameters:\n",
      "{'K': 7, 'Distance': 'HEOM', 'Voting scheme': 'Majority_class', 'Weight scheme': 'Mutual_classifier'}\n",
      "Best Accuracy: 0.9626\n",
      "Friedman test statistic: 0.3773584905660699, p-value: 0.9842883045217616\n",
      "No significant differences found between the models.\n",
      "\n",
      "\n",
      "Evaluating method svm on dataset: sick\n",
      "Best Hyperparameters:\n",
      "{'Kernel': 'rbf'}\n",
      "Best Accuracy: 0.9594\n",
      "Wilcoxon signed-rank test statistic: 1.0, p-value: 0.017153601700200603\n",
      "Significant difference between the two models.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read_csvs\n",
    "\n",
    "dataset_names = ['grid','sick']\n",
    "methods = ['knn','svm']\n",
    "best_params = {}\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    best_params[dataset_name] = {}\n",
    "    for method in methods:\n",
    "        print(f'Evaluating method {method} on dataset: {dataset_name}')\n",
    "        df_combined = read_csv(f'results_{method}', dataset_name, False)\n",
    "        metrics_summary, best_hyperparams = aggregate_metrics(df_combined, method)\n",
    "        metrics_summary.to_csv(f'results_{method}/results_{dataset_name}_all.csv', index=False)\n",
    "        best_params[dataset_name][method] = best_hyperparams\n",
    "        best_models = metrics_summary.head(5)\n",
    "        if method == 'knn':\n",
    "            metric_values = get_metrics_knn(best_models, df_combined)\n",
    "            best_params\n",
    "        elif method == 'svm':\n",
    "            metric_values = get_metrics_svm(best_models, df_combined)\n",
    "        evaluation_test(metric_values)\n",
    "        print('\\n')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7bb2687b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "{'K': 7, 'Distance': 'minkowski2', 'Voting scheme': 'Majority_class', 'Weight scheme': 'Mutual_classifier'}\n",
      "Best Accuracy: 0.9672\n",
      "\n",
      "Comparing method knn on dataset grid with reduction CNN\n",
      "Best Hyperparameters:\n",
      "{'K': 7, 'Distance': 'minkowski2', 'Voting scheme': 'Majority_class', 'Weight scheme': 'Mutual_classifier'}\n",
      "Best Accuracy: 0.6166\n",
      "Paired t-test statistic: 10.148441037251782, p-value: 3.1647943713369135e-06\n",
      "Significant difference found between the two models.\n",
      "\n",
      "Comparing method knn on dataset grid with reduction DROP\n",
      "Best Hyperparameters:\n",
      "{'K': 7, 'Distance': 'minkowski2', 'Voting scheme': 'Majority_class', 'Weight scheme': 'Mutual_classifier'}\n",
      "Best Accuracy: 0.9295\n",
      "Paired t-test statistic: 13.417601525242189, p-value: 2.9586843076967304e-07\n",
      "Significant difference found between the two models.\n",
      "\n",
      "Comparing method knn on dataset grid with reduction EENTh\n",
      "Best Hyperparameters:\n",
      "{'K': 7, 'Distance': 'minkowski2', 'Voting scheme': 'Majority_class', 'Weight scheme': 'Mutual_classifier'}\n",
      "Best Accuracy: 0.9719\n",
      "Paired t-test statistic: -1.1721444211417325, p-value: 0.2712313435223187\n",
      "No significant differences found between the two models.\n",
      "Best Hyperparameters:\n",
      "{'Kernel': 'rbf'}\n",
      "Best Accuracy: 0.9142\n",
      "\n",
      "Comparing method svm on dataset grid with reduction CNN\n",
      "Best Hyperparameters:\n",
      "{'Kernel': 'rbf'}\n",
      "Best Accuracy: 0.8543\n",
      "Paired t-test statistic: 8.176538881610616, p-value: 1.858483688976833e-05\n",
      "Significant difference found between the two models.\n",
      "\n",
      "Comparing method svm on dataset grid with reduction DROP\n",
      "Best Hyperparameters:\n",
      "{'Kernel': 'rbf'}\n",
      "Best Accuracy: 0.8893\n",
      "Paired t-test statistic: 5.411099879631341, p-value: 0.00042676930395011445\n",
      "Significant difference found between the two models.\n",
      "\n",
      "Comparing method svm on dataset grid with reduction EENTh\n",
      "Best Hyperparameters:\n",
      "{'Kernel': 'rbf'}\n",
      "Best Accuracy: 0.9147\n",
      "Paired t-test statistic: -0.555048213113138, p-value: 0.592386937754485\n",
      "No significant differences found between the two models.\n"
     ]
    }
   ],
   "source": [
    "dataframe_best_results_together = {}\n",
    "\n",
    "num_samples = {'grid': 1700, 'sick': 3395}\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    dataframe_best_results_together[dataset_name] = pd.DataFrame()\n",
    "    for method in methods:\n",
    "        df_combined = read_csv(f'results_{method}', dataset_name, False)\n",
    "        metrics_summary, best_hyperparams = aggregate_metrics(df_combined, method)\n",
    "        \n",
    "        # Make a copy of the best_models DataFrame to avoid SettingWithCopyWarning\n",
    "        best_models = metrics_summary.head(5).copy()\n",
    "        \n",
    "        if method == 'knn':\n",
    "            metric_values = get_metrics_knn(best_models, df_combined)[0]\n",
    "        elif method == 'svm':\n",
    "            metric_values = get_metrics_svm(best_models, df_combined)[0]\n",
    "        \n",
    "        best_models.loc[:, 'Method'] = method\n",
    "        best_models.loc[:, 'Num samples'] = num_samples[dataset_name]\n",
    "        dataframe_best_results_together[dataset_name] = pd.concat((dataframe_best_results_together[dataset_name], best_models.head(1)))\n",
    "        \n",
    "        for reduction in ['CNN', 'DROP', 'EENTh']:\n",
    "\n",
    "            print(f'\\nComparing method {method} on dataset {dataset_name} with reduction {reduction}')\n",
    "            results_reduced = read_csv(f'results_{method}_reduced', dataset_name, reduction)\n",
    "            metrics_summary, best_hyperparams = aggregate_metrics(results_reduced, method)\n",
    "            \n",
    "            # Make a copy of the best_models DataFrame to avoid SettingWithCopyWarning\n",
    "            best_models = metrics_summary.head(5).copy()\n",
    "            \n",
    "            best_models.loc[:, 'Method'] = method\n",
    "            best_models.loc[:, 'reduction'] = reduction\n",
    "            best_models.loc[:, 'Num samples'] = results_reduced['Num samples']\n",
    "            \n",
    "            dataframe_best_results_together[dataset_name] = pd.concat((dataframe_best_results_together[dataset_name], best_models.head(1)))\n",
    "            \n",
    "            metrics_summary.to_csv(f'results_{method}_reduced/results_{dataset_name}_{reduction}_all.csv', index=False)\n",
    "            metric_values_reduced = results_reduced['Accuracy'].values\n",
    "            \n",
    "            evaluation_t_test(metric_values, metric_values_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5351fff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy_mean</th>\n",
       "      <th>Num samples</th>\n",
       "      <th>Solving Time_mean</th>\n",
       "      <th>reduction</th>\n",
       "      <th>Method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.967162</td>\n",
       "      <td>1700</td>\n",
       "      <td>3.240735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>knn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.616602</td>\n",
       "      <td>130</td>\n",
       "      <td>0.365052</td>\n",
       "      <td>CNN</td>\n",
       "      <td>knn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.929545</td>\n",
       "      <td>341</td>\n",
       "      <td>0.843815</td>\n",
       "      <td>DROP</td>\n",
       "      <td>knn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.971910</td>\n",
       "      <td>1552</td>\n",
       "      <td>3.357328</td>\n",
       "      <td>EENTh</td>\n",
       "      <td>knn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.914167</td>\n",
       "      <td>1700</td>\n",
       "      <td>0.284618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>svm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.854294</td>\n",
       "      <td>130</td>\n",
       "      <td>0.011683</td>\n",
       "      <td>CNN</td>\n",
       "      <td>svm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.889282</td>\n",
       "      <td>341</td>\n",
       "      <td>0.017874</td>\n",
       "      <td>DROP</td>\n",
       "      <td>svm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.914696</td>\n",
       "      <td>1552</td>\n",
       "      <td>0.160981</td>\n",
       "      <td>EENTh</td>\n",
       "      <td>svm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy_mean  Num samples  Solving Time_mean reduction Method\n",
       "103       0.967162         1700           3.240735       NaN    knn\n",
       "0         0.616602          130           0.365052       CNN    knn\n",
       "0         0.929545          341           0.843815      DROP    knn\n",
       "0         0.971910         1552           3.357328     EENTh    knn\n",
       "0         0.914167         1700           0.284618       NaN    svm\n",
       "0         0.854294          130           0.011683       CNN    svm\n",
       "0         0.889282          341           0.017874      DROP    svm\n",
       "0         0.914696         1552           0.160981     EENTh    svm"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['Accuracy_mean', 'Num samples', 'Solving Time_mean', 'reduction', 'Method'] \n",
    "dataframe_best_results_together['grid'][columns]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
